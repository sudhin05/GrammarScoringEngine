{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfadb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_no(filename):\n",
    "    match = re.search(r'audio_(\\d+)\\.wav', filename)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "def get_audio_info(audio_dir):\n",
    "    data = []\n",
    "    files = [f for f in os.listdir(audio_dir) if f.endswith('.wav')]\n",
    "    files = sorted(files, key=extract_no)\n",
    "\n",
    "    for file in files:\n",
    "        path = os.path.join(audio_dir, file)\n",
    "        try:\n",
    "            y, sr = librosa.load(path, sr=None)\n",
    "            duration = librosa.get_duration(y=y, sr=sr)\n",
    "            data.append({'filename': file, 'sample_rate': sr, 'duration_sec': duration})\n",
    "        except Exception as e:\n",
    "            print(f\"Error hai in {file}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def plot_aud_distri(df,col='duration_sec'):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df[col], bins=50, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distri of aud file {col}')\n",
    "    plt.xlabel(f'{col})')\n",
    "    plt.ylabel('Number of Files')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def flag_audio_info(df,csv_path):\n",
    "    df_long = df[df['duration_sec'] > 70]\n",
    "    df_long_sorted = df_long.sort_values(by='duration_sec', ascending=True)\n",
    "\n",
    "    df_long_sorted.to_csv(csv_path, index=False)\n",
    "    print(f\"Saved {len(df_long_sorted)} long files to {csv_path}\")\n",
    "\n",
    "def mod_flag_audio_info(df_all,df_flagged,csv_path):\n",
    "    df_merged = pd.merge(df_flagged, df_all[['filename', 'label']], on='filename', how='left')\n",
    "    df_merged.to_csv(csv_path, index=False)\n",
    "    print(f\"Merged CSV saved as: {csv_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_dir = 'Dataset/audios/train' \n",
    "    labels_csv = 'Dataset/train.csv'\n",
    "    output_csv = 'logs/csvs/audio_prop.csv'\n",
    "    output2_csv = 'logs/csvs/flag_audio_prop.csv'\n",
    "    output3_csv = 'logs/csvs/modflags_audio_prop.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c49e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_audio_info(audio_dir)\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"Metadata saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dccce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv)\n",
    "plot_aud_distri(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc87e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_csv)\n",
    "flag_audio_info(df,output2_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c493a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(labels_csv)\n",
    "df_flagged = pd.read_csv(output2_csv)\n",
    "mod_flag_audio_info(df_all,df_flagged,output3_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dd77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_mod = pd.read_csv(output3_csv)\n",
    "plot_aud_distri(df_mod,col='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac5eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import librosa\n",
    "from faster_whisper import WhisperModel\n",
    "\n",
    "grammar_rubric = {\n",
    "    1.0: \"The person's speech struggles with proper sentence structure and syntax, displaying limited control over simple grammatical structures and memorized sentence patterns.\",\n",
    "    1.5: \"The person's speech shows signs of basic structure but still has notable issues with grammar and syntax.\",\n",
    "    2.0: \"The person has a limited understanding of sentence structure and syntax. Although they use simple structures, they consistently make basic sentence structure and grammatical mistakes. They might leave sentences incomplete.\",\n",
    "    2.5: \"The person sometimes forms correct sentences but often makes errors that affect understanding.\",\n",
    "    3.0: \"The person demonstrates a decent grasp of sentence structure but makes errors in grammatical structure, or they show a decent grasp of grammatical structure but make errors in sentence syntax and structure.\",\n",
    "    3.5: \"The person has mostly correct grammar but makes occasional errors that may slightly affect clarity.\",\n",
    "    4.0: \"The person displays a strong understanding of sentence structure and syntax. They consistently show good control of grammar. While occasional errors may occur, they are generally minor and do not lead to misunderstandings; the person can correct most of them.\",\n",
    "    4.5: \"The person speaks accurately most of the time, with small grammar issues that rarely affect clarity.\",\n",
    "    5.0: \"Overall, the person showcases high grammatical accuracy and adept control of complex grammar. They use grammar accurately and effectively, seldom making noticeable mistakes. Additionally, they handle complex language structures well and correct themselves when necessary.\"\n",
    "}\n",
    "\n",
    "label_csv = \"Dataset/test.csv\"\n",
    "audio_dir = \"Dataset/audios/test/\"\n",
    "output_csv = \"testing_data.csv\"\n",
    "\n",
    "df = pd.read_csv(label_csv)\n",
    "model = WhisperModel(\"large\", compute_type=\"float16\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    # label = float(row[\"label\"])\n",
    "\n",
    "    path = os.path.join(audio_dir, filename)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[WARN] File not found: {path}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        y, sr = librosa.load(path, sr=16000)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        if duration > 70:\n",
    "            rows.append({\n",
    "                \"filename\": filename,\n",
    "                \"transcription\": \"flag\"\n",
    "            })\n",
    "            print(f\"[FLAG] {filename} duration {duration:.2f}s marked as 'flag'\")\n",
    "            continue\n",
    "\n",
    "        full_transcription = \"\"\n",
    "\n",
    "        # Process in 20s chunks\n",
    "        chunk_length = 20 * sr\n",
    "        total_chunks = int(len(y) / chunk_length) + 1\n",
    "\n",
    "        for i in range(total_chunks):\n",
    "            start = i * chunk_length\n",
    "            end = min((i + 1) * chunk_length, len(y))\n",
    "            chunk = y[start:end]\n",
    "\n",
    "            if len(chunk) < sr:\n",
    "                continue\n",
    "\n",
    "            segments, _ = model.transcribe(chunk, language=\"en\", beam_size=5)\n",
    "            chunk_text = \" \".join([seg.text for seg in segments]).strip()\n",
    "            full_transcription += \" \" + chunk_text\n",
    "\n",
    "        # rubric_desc = grammar_rubric.get(label, \"Unknown rubric description.\")\n",
    "\n",
    "        rows.append({\n",
    "            \"filename\": filename,\n",
    "            \"transcription\": full_transcription.strip(),\n",
    "        })\n",
    "\n",
    "        print(f\"[OK] Processed {filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[ERR] Failed {filename}: {e}\")\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "df_out.to_csv(output_csv, index=False)\n",
    "print(f\"\\nSaved {len(df_out)} entries to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82818181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, DataCollatorWithPadding\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"training_data_for_t5.csv\")  \n",
    "\n",
    "df = df.dropna(subset=[\"transcription\", \"label\"])\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=1,\n",
    "    problem_type=\"regression\"  \n",
    ")\n",
    "\n",
    "def preprocess(batch):\n",
    "    tokens = tokenizer(\n",
    "        batch[\"transcription\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    tokens[\"labels\"] = [float(l) for l in batch[\"label\"]]\n",
    "    return tokens\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_grammar_regressor\",\n",
    "    num_train_epochs=50,             \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,            \n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Data collator to pad \n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "save_dir = \"./saved_bert_grammar_regressor\"\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Model saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "INPUT_CSV = \"testing_data.csv\"\n",
    "OUTPUT_CSV = \"submission2.csv\"\n",
    "MODEL_DIR = \"./saved_bert_grammar_regressor\" \n",
    "MAX_LENGTH = 256\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "\n",
    "valid_scores = np.array([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "\n",
    "def round_to_rubric(pred: float) -> float:\n",
    "    pred = min(max(pred, 1.0), 5.0)\n",
    "    closest = valid_scores[np.argmin(np.abs(valid_scores - pred))]\n",
    "    return closest\n",
    "\n",
    "def get_flag_score():\n",
    "    scores = [5.0, 4.5, 4.0]\n",
    "    weights = [0.85, 0.1, 0.05]\n",
    "    return 5.0\n",
    "\n",
    "def predict_score(transcription: str) -> float:\n",
    "    if transcription.strip().lower() == \"flag\":\n",
    "        return get_flag_score()\n",
    "    \n",
    "    inputs = tokenizer(transcription, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        raw_score = output.logits.squeeze().item()\n",
    "        final_score = round_to_rubric(raw_score)\n",
    "        return final_score\n",
    "\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "df[\"label\"] = df[\"transcription\"].apply(predict_score)\n",
    "\n",
    "df[[\"filename\", \"label\"]].to_csv(OUTPUT_CSV, index=False)\n",
    "print(\"Saved predictions to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
